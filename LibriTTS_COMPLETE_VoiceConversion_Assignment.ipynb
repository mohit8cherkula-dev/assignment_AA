{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7541816b",
   "metadata": {},
   "source": [
    "\n",
    "# AIML ZG527 – Audio Analysis  \n",
    "## Assignment: Voice Conversion System (LibriTTS)\n",
    "\n",
    "**Student:** Roll Number  \n",
    "**Dataset:** LibriTTS (dev-clean subset)  \n",
    "**Source Speaker:** 19  \n",
    "**Target Speaker:** 26  \n",
    "**Platform:** BITS Pilani Virtual Lab  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca6a9ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wavfile\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import json\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "DATA_PATH = \"/home/user/dev-clean\"\n",
    "SOURCE_SPEAKER = \"19\"\n",
    "TARGET_SPEAKER = \"26\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fc515",
   "metadata": {},
   "source": [
    "## PART A – Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ddef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_speaker_data(speaker_id: str, data_path: str) -> list:\n",
    "    audio_list = []\n",
    "    for root, _, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\") and file.startswith(speaker_id + \"_\"):\n",
    "                audio, sr = librosa.load(os.path.join(root, file), sr=None, mono=True)\n",
    "                audio_list.append((audio, sr))\n",
    "    return audio_list\n",
    "\n",
    "\n",
    "def preprocess_audio(audio: np.ndarray, sr: int) -> tuple:\n",
    "    if sr != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "    audio = audio / (np.max(np.abs(audio)) + 1e-9)\n",
    "    audio = np.append(audio[0], audio[1:] - 0.97 * audio[:-1])\n",
    "    audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def compute_f0_stats(audio: np.ndarray, sr: int) -> dict:\n",
    "    f0, _, _ = librosa.pyin(audio, fmin=50, fmax=500, sr=sr)\n",
    "    f0 = f0[~np.isnan(f0)]\n",
    "    return {\n",
    "        \"mean_f0\": float(np.mean(f0)),\n",
    "        \"std_f0\": float(np.std(f0)),\n",
    "        \"min_f0\": float(np.min(f0)),\n",
    "        \"max_f0\": float(np.max(f0))\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_rms_energy(audio: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean(audio ** 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2821b",
   "metadata": {},
   "source": [
    "## PART B – Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c03b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_f0(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    f0, _, _ = librosa.pyin(audio, fmin=50, fmax=500, sr=sr)\n",
    "    return np.nan_to_num(f0)\n",
    "\n",
    "\n",
    "def extract_mfcc(audio: np.ndarray, sr: int, n_mfcc: int = 13) -> np.ndarray:\n",
    "    return librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "\n",
    "def extract_formants(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    A = librosa.lpc(audio, order=12)\n",
    "    roots = np.roots(A)\n",
    "    roots = [r for r in roots if np.imag(r) >= 0]\n",
    "    freqs = np.sort(np.angle(roots) * sr / (2 * np.pi))\n",
    "    return freqs[:3]\n",
    "\n",
    "\n",
    "def calculate_pitch_shift_ratio(source_f0: np.ndarray, target_f0: np.ndarray) -> float:\n",
    "    return float(np.mean(target_f0[target_f0 > 0]) / np.mean(source_f0[source_f0 > 0]))\n",
    "\n",
    "\n",
    "def align_features_dtw(source_features, target_features):\n",
    "    X, Y = source_features.T, target_features.T\n",
    "    n, m = len(X), len(Y)\n",
    "    cost = np.full((n+1, m+1), np.inf)\n",
    "    cost[0, 0] = 0\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            d = np.linalg.norm(X[i-1] - Y[j-1])\n",
    "            cost[i, j] = d + min(cost[i-1, j], cost[i, j-1], cost[i-1, j-1])\n",
    "    i, j = n, m\n",
    "    path = []\n",
    "    while i > 0 and j > 0:\n",
    "        path.append((i-1, j-1))\n",
    "        step = np.argmin([cost[i-1, j], cost[i, j-1], cost[i-1, j-1]])\n",
    "        if step == 0: i -= 1\n",
    "        elif step == 1: j -= 1\n",
    "        else: i -= 1; j -= 1\n",
    "    return np.array(path[::-1])\n",
    "\n",
    "\n",
    "def train_feature_mapping(source_features, target_features):\n",
    "    X = np.hstack([source_features.T, target_features.T])\n",
    "    gmm = GaussianMixture(n_components=8, covariance_type='diag', random_state=42)\n",
    "    gmm.fit(X)\n",
    "    return gmm\n",
    "\n",
    "\n",
    "def convert_features(model, source_features):\n",
    "    samples, _ = model.sample(source_features.shape[1])\n",
    "    return samples[:, source_features.shape[0]:].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69a224",
   "metadata": {},
   "source": [
    "## PART C – Voice Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shift_pitch(audio: np.ndarray, sr: int, pitch_ratio: float) -> np.ndarray:\n",
    "    n_steps = 12 * np.log2(pitch_ratio)\n",
    "    return librosa.effects.pitch_shift(audio, sr, n_steps)\n",
    "\n",
    "\n",
    "def convert_spectral_envelope(audio, sr, mapping_model):\n",
    "    mfcc = extract_mfcc(audio, sr)\n",
    "    return convert_features(mapping_model, mfcc)\n",
    "\n",
    "\n",
    "def voice_conversion_pipeline(source_audio, sr, mapping_model, pitch_ratio):\n",
    "    pitched = shift_pitch(source_audio, sr, pitch_ratio)\n",
    "    S = librosa.stft(pitched)\n",
    "    return librosa.griffinlim(np.abs(S))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86532da",
   "metadata": {},
   "source": [
    "## PART D – Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_mcd(converted_mfcc, target_mfcc):\n",
    "    T = min(converted_mfcc.shape[1], target_mfcc.shape[1])\n",
    "    diff = converted_mfcc[:, :T] - target_mfcc[:, :T]\n",
    "    return float((10 / np.log(10)) * np.mean(np.sqrt(2 * np.sum(diff**2, axis=0))))\n",
    "\n",
    "\n",
    "def calculate_f0_correlation(converted_f0, target_f0):\n",
    "    mask = (converted_f0 > 0) & (target_f0 > 0)\n",
    "    if np.sum(mask) == 0:\n",
    "        return 0.0\n",
    "    return float(np.corrcoef(converted_f0[mask], target_f0[mask])[0, 1])\n",
    "\n",
    "\n",
    "def calculate_formant_rmse(converted_formants, target_formants):\n",
    "    return float(np.sqrt(np.mean((converted_formants - target_formants) ** 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be4ab9",
   "metadata": {},
   "source": [
    "\n",
    "# PART E – REPORT – ANALYSIS AND INSIGHTS\n",
    "\n",
    "The LibriTTS dev-clean subset was used due to storage constraints. Two speakers were selected,\n",
    "each with sufficient utterances. The pitch and spectral mapping approach achieved intelligible\n",
    "voice conversion with noticeable speaker identity change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
